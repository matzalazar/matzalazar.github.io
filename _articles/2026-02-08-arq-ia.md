---
layout: post
title: "Arquitectura de IA para procesamiento de video"
date: 2026-02-08
category: ciberseguridad
description: "Arquitectura en cascada: filtros baratos primero y VLMs caros solo sobre candidatos para escalar análisis visual."
project: "Vigilant"
project_part: 2
project_total: 3
---

# Estrategias de diseño para minimizar costo de inferencia en flujos de análisis visual

En análisis visual automatizado —especialmente en contextos forenses o de grandes volúmenes de video— el problema principal rara vez es el costo computacional.

Aplicar modelos multimodales o generativos sobre cada frame de un video completo suele ser inviable:

- latencias de segundos por frame    
- consumo elevado de GPU/VRAM
- tiempos totales que escalan linealmente con la duración del material

El resultado práctico es que el pipeline no escala.

La solución no suele estar en “optimizar el modelo”, sino en **cambiar la arquitectura**.

Este artículo describe un patrón de diseño simple y efectivo:

> filtrar barato primero, pagar caro solo cuando hay alta probabilidad de match.

Una **arquitectura en cascada**.

No depende de herramientas específicas.  
Es una estrategia general aplicable a cualquier stack de visión por computadora.

## 1. El problema fundamental: la inferencia no cuesta lo mismo

No todos los modelos tienen el mismo costo.

En la práctica conviven dos categorías muy distintas:

### Detectores ligeros

Ejemplos típicos:

- YOLO
- MobileNet-SSD
- detectores clásicos CNN

Propiedades:

- latencia baja y predecible
- cientos de FPS posibles
- responden preguntas simples (“hay persona/auto/moto”)
- salida estructurada (bounding boxes + score)

### Modelos multimodales / generativos

Ejemplos:

- LLaVA
- VLMs (Vision Language Models)
- captioning/QA visual

Propiedades:

- latencia muy alta
- segundos por frame no es raro
- flexibilidad enorme (preguntas complejas, semánticas)
- comportamiento menos determinista

El error común es tratar ambos como intercambiables. No lo son.

Un VLM sobre todo el video es, en la práctica, prohibitivo.

## 2. El error típico: inferencia plana

El diseño más ingenuo es: para cada frame → ejecutar modelo complejo

Costo total:

`costo_total ≈ N_frames × costo_modelo_caro`

Si:

- 10.000 frames
- 5–10 s por inferencia

El procesamiento puede demorar horas o incluso días.

Técnicamente funciona.  
Operativamente no.

## 3. La idea central: pensar como un embudo

La intuición correcta debe ser económica desde el punto de vista computacional. No todos los frames valen lo mismo.

La mayoría son:

- fondo estático
- escenas vacías
- repeticiones sin información nueva

Solo un subconjunto pequeño contiene eventos relevantes.

Entonces tiene sentido:

> gastar poco en todos, mucho en pocos.

Eso lleva naturalmente a una arquitectura en cascada.

## 4. Arquitectura en cascada

El pipeline deja de ser plano y pasa a ser por etapas.

### Etapa 1 — Reducción barata

Filtrado rápido y masivo.

Ejemplos:

- muestreo temporal (1 frame cada N segundos)    
- downscale de resolución
- detección de movimiento
- detectores ligeros de objetos

Objetivo: Descartar la mayor cantidad posible con costo mínimo.

### Etapa 2 — Validación intermedia

Análisis un poco más caro, pero aún barato comparado con modelos grandes.

Ejemplos:

- reglas geométricas    
- tracking simple
- consistencia temporal
- embeddings livianos

Objetivo: Reducir falsos positivos antes de pagar el costo grande.

### Etapa 3 — Análisis profundo

Solo sobre candidatos.

Ejemplos:

- VLMs    
- captioning
- reasoning multimodal
- modelos generativos

Objetivo: Extraer semántica rica solo donde tiene sentido.

Conceptualmente:

frames → filtro barato → pocos candidatos → modelo caro

Es un embudo.

## 5. Por qué esto funciona (matemáticamente)

Supongamos:

- 10.000 frames
- detector barato: 0.01 s
- modelo caro: 8 s
- solo 2% pasan el filtro

### Sin cascada

`10.000 × 8 s = 80.000 s`

### Con cascada

`10.000 × 0.01 s + 200 × 8 s = 1.700 s`

La diferencia es órdenes de magnitud.

El modelo caro sigue siendo caro.  
Simplemente se usa menos.

## 6. Técnicas de filtrado habituales

Estas técnicas son independientes de cualquier framework.

### Muestreo temporal

Procesar cada N frames.
Reduce linealmente el costo.
Trade-off: eventos muy breves pueden perderse.

### Downscaling

Reducir resolución antes del detector.
Acelera CNNs dramáticamente.
Trade-off: objetos pequeños.

### Detección de movimiento

Comparar frames consecutivos.
Descarta escenas estáticas.
Especialmente útil en CCTV.

### Detectores de clases simples

Personas, vehículos, motos, etc.
Si no hay clase relevante, no tiene sentido análisis profundo.

### Embeddings livianos

Similitud semántica rápida antes de pasar a modelos generativos.
Sirve como filtro intermedio.

## 7. Controles que realmente importan

En producción, los “knobs” clave no son el modelo, sino:

- intervalo de muestreo
- resolución
- umbrales de confianza
- tamaño máximo de candidatos
- límite de inferencias caras por minuto

Estos parámetros gobiernan:

precisión ↔ costo ↔ tiempo total

Es un problema de ingeniería de sistemas, no de ML puro.

La cascada ayuda porque hace explícitas las etapas y criterios.

## 8. Límites reales

La cascada no es mágica.

Riesgos:

- intervalos altos → pérdida de eventos cortos
- detectores simples → sesgo de clases
- modelos generativos → no determinismo

Es un trade-off controlado, no una garantía absoluta.

## 9. Conclusión

Cuando el costo de inferencia es alto, la optimización suele estar en el diseño de la arquitectura.

Procesar todo con modelos pesados es conceptualmente simple, pero económicamente inviable.

La estrategia más robusta es:

- filtrar barato
- reducir el problema
- pagar caro solo donde importa

Y en sistemas forenses o de gran volumen, suele ser la diferencia entre algo teóricamente posible y algo realmente usable.
